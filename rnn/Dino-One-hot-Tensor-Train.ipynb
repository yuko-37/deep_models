{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8351ab1-166c-4518-b772-3b84bc45990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# print(tf.__version__)\n",
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e03fc7-9afa-4110-8230-4a7e1699ee6e",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff6fbdc-d78c-4159-b56d-d14f295fa39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabular [27 chars]: ['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "maxlen = 26, dino=lisboasaurusliubangosaurus, idx = 791\n",
      "\n",
      "char_to_ix = {'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "\n",
      "ix_to_char = {0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "\n",
      "len(inputs) = 1536, len(outputs) = 1536\n",
      "\n",
      "inputs[200] = [None, 2, 9, 5, 14, 15, 19, 1, 21, 18, 21, 19]\n",
      "outputs[200] = [2, 9, 5, 14, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "\n",
      "X_padded.shape = (1536, 27, 27)\n",
      "Y_padded.shape = (1536, 27, 27)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"dinos.txt\", \"r\") as f:\n",
    "    dinos_str = f.read().lower()\n",
    "\n",
    "vocabular = sorted(set(dinos_str))\n",
    "print(f\"Vocabular [{len(vocabular)} chars]: {vocabular}\\n\")\n",
    "\n",
    "dinos = [ds.strip() for ds in dinos_str.split(\"\\n\")]\n",
    "lens = [(len(d), d) for d in dinos]\n",
    "maxlen, d = max(lens)\n",
    "maxidx = lens.index((maxlen, d))\n",
    "print(f\"maxlen = {maxlen}, dino={d}, idx = {maxidx}\\n\")\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(vocabular) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(vocabular) }\n",
    "\n",
    "print(f\"char_to_ix = {char_to_ix}\\n\")\n",
    "print(f\"ix_to_char = {ix_to_char}\\n\")\n",
    "\n",
    "n_vocab_size = 27 # 26 lower english letters + \\n\n",
    "n_a = 50 # number of state units\n",
    "\n",
    "inputs = [[None] + [char_to_ix[char] for char in dino] for dino in dinos]\n",
    "outputs = [x[1:] + [0] for x in inputs]\n",
    "print(f\"len(inputs) = {len(inputs)}, len(outputs) = {len(outputs)}\\n\")\n",
    "print(f\"inputs[200] = {inputs[200]}\")\n",
    "print(f\"outputs[200] = {outputs[200]}\\n\")\n",
    "\n",
    "\n",
    "def indexes_to_one_hot_vectors(inputs, ohv_dim):\n",
    "    vectors = []\n",
    "    for item in inputs:\n",
    "        vector = np.zeros((len(item), ohv_dim))\n",
    "        \n",
    "        for i, idx in enumerate(item):\n",
    "            if idx is None:\n",
    "                vector[i] = [0] * ohv_dim\n",
    "            else:\n",
    "                vector[i][idx] = 1\n",
    "\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    return vectors\n",
    "\n",
    "\n",
    "x_sequences = indexes_to_one_hot_vectors(inputs, n_vocab_size)\n",
    "y_sequences = indexes_to_one_hot_vectors(outputs, n_vocab_size)\n",
    "\n",
    "X_padded = tf.keras.utils.pad_sequences(x_sequences[:], value=-1.0, padding='post', dtype='float32')\n",
    "Y_padded = tf.keras.utils.pad_sequences(y_sequences[:], value=-1.0, padding='post', dtype='float32')\n",
    "\n",
    "print(f\"X_padded.shape = {X_padded.shape}\")\n",
    "print(f\"Y_padded.shape = {Y_padded.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9cfd32-15d4-4fb7-9ca0-10764a4909ec",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf3526e-412c-47b2-aeab-55c741a8a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, newline_char=0, max_len=27):\n",
    "    counter = 0\n",
    "    indices = []\n",
    "    x = np.zeros((1, n_vocab_size))\n",
    "    idx = -1\n",
    "    state = None\n",
    "    \n",
    "    while idx != newline_char and counter < max_len:\n",
    "        y_pred = model.predict(np.expand_dims(x, axis=0), verbose=0)\n",
    "        probs = y_pred[0, -1, :]\n",
    "        idx = np.random.choice(range(len(probs)), p=probs)\n",
    "        if idx == 0:\n",
    "            break\n",
    "        indices.append(idx)\n",
    "        new_x = np.zeros((n_vocab_size,))\n",
    "        new_x[idx] = 1.0\n",
    "        x = np.vstack([x, new_x])\n",
    "        counter+=1\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_sample(model):\n",
    "    indices = sample_sequence(model)\n",
    "    name = \"\".join([ix_to_char[i] for i in indices])\n",
    "    return name\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4e0cd-6ae4-466b-bd3f-b68788d76998",
   "metadata": {},
   "source": [
    "# Model RNN with Masking & Train on Tensor Padded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6f08a8-d026-4725-b54e-e0f6b6ee82d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 | Loss: 133.52357482910156 | Avr Loss: 2.781741142272949 | Num of Batches: 48 | Accuracy: 0.2059769183397293\n",
      "Oxitncaus\n",
      "Sanpus\n",
      "S\n",
      "Aov\n",
      "Tisltlaccibyekqals\n",
      "Ssnducunh\n",
      "Cnc\n",
      "\n",
      "\n",
      "Epoch 2 | Loss: 107.12640380859375 | Avr Loss: 2.231800079345703 | Num of Batches: 48 | Accuracy: 0.2855599820613861\n",
      "Tanyourus\n",
      "Aurtivihbuemur\n",
      "Auranydan\n",
      "Ssinotia\n",
      "Himusup\n",
      "Gitninutosausurnurus\n",
      "Ranipnusiurus\n",
      "\n",
      "\n",
      "Epoch 3 | Loss: 95.86166381835938 | Avr Loss: 1.9971179962158203 | Num of Batches: 48 | Accuracy: 0.3295495808124542\n",
      "Tanociiceicus\n",
      "Rhariloscelicis\n",
      "Onetetorostodo\n",
      "Ereeusaurus\n",
      "Wiclanosandipsaurus\n",
      "Pinhtnraniur\n",
      "Ihohtepa\n",
      "\n",
      "\n",
      "Epoch 4 | Loss: 91.43601989746094 | Avr Loss: 1.9049171209335327 | Num of Batches: 48 | Accuracy: 0.3547587990760803\n",
      "Enaltaur\n",
      "Ecenanus\n",
      "Plhanxis\n",
      "Pvraxoxus\n",
      "Fralisaurus\n",
      "Kiavenymaentavon\n",
      "Lalerui\n",
      "\n",
      "\n",
      "Epoch 5 | Loss: 88.9822006225586 | Avr Loss: 1.8537958860397339 | Num of Batches: 48 | Accuracy: 0.37310895323753357\n",
      "Con\n",
      "Kegetie\n",
      "Silichosaurus\n",
      "Finhinsaurus\n",
      "Ssinter\n",
      "Dslopesauruc\n",
      "Santenpa\n",
      "\n",
      "\n",
      "Epoch 6 | Loss: 86.93382263183594 | Avr Loss: 1.8111213445663452 | Num of Batches: 48 | Accuracy: 0.38703322410583496\n",
      "Ilitomasuinus\n",
      "Monisaurus\n",
      "Fupuceratops\n",
      "Uthivhuvinus\n",
      "Tunathumus\n",
      "Amapannus\n",
      "Cechoehemya\n",
      "\n",
      "\n",
      "Epoch 7 | Loss: 85.232421875 | Avr Loss: 1.7756754159927368 | Num of Batches: 48 | Accuracy: 0.39843568205833435\n",
      "Lapahuracups\n",
      "Vurazrolimisaurus\n",
      "Minuspus\n",
      "Goanostengtendeos\n",
      "Muyvacor\n",
      "Surocel\n",
      "Keroceratoo\n",
      "\n",
      "\n",
      "Epoch 8 | Loss: 83.84481811523438 | Avr Loss: 1.7467670440673828 | Num of Batches: 48 | Accuracy: 0.4081050753593445\n",
      "Smialia\n",
      "Zonoraanivucerosaurus\n",
      "Proceratons\n",
      "Drotharus\n",
      "Jcecsyuanosaurus\n",
      "Shusvornon\n",
      "Cerchennia\n",
      "\n",
      "\n",
      "Epoch 9 | Loss: 82.76766204833984 | Avr Loss: 1.724326252937317 | Num of Batches: 48 | Accuracy: 0.4162618815898895\n",
      "Janhiraptor\n",
      "Dritrisaurus\n",
      "Vuphiroys\n",
      "Ernorax\n",
      "Henorodon\n",
      "Landitaura\n",
      "Shinosaurus\n",
      "\n",
      "\n",
      "Epoch 10 | Loss: 82.10721588134766 | Avr Loss: 1.7105669975280762 | Num of Batches: 48 | Accuracy: 0.4232392907142639\n",
      "Ngorgan\n",
      "Trotlohaiisaurus\n",
      "Maramodingoisaurus\n",
      "Criatrnasiobyton\n",
      "Pharaetixia\n",
      "Sartesaura\n",
      "Dogosaurus\n",
      "\n",
      "\n",
      "Epoch 11 | Loss: 81.52259826660156 | Avr Loss: 1.6983875036239624 | Num of Batches: 48 | Accuracy: 0.4292038679122925\n",
      "Saugoxiusnurnosaurus\n",
      "Doluoventcrosnitetcapiapzhu\n",
      "Ruracaceriusourus\n",
      "Hiabodon\n",
      "Shilopasaurus\n",
      "Pieoukukuenonyulurus\n",
      "Ruenlhongoenricema\n",
      "\n",
      "\n",
      "Epoch 12 | Loss: 80.8219223022461 | Avr Loss: 1.6837900876998901 | Num of Batches: 48 | Accuracy: 0.43444663286209106\n",
      "Wonovenator\n",
      "Wepromelus\n",
      "Lowsushumosaurus\n",
      "Ptotesaurus\n",
      "Tygosaurus\n",
      "Reirinathehus\n",
      "Liaprobnia\n",
      "\n",
      "\n",
      "Epoch 13 | Loss: 79.82696533203125 | Avr Loss: 1.6630617380142212 | Num of Batches: 48 | Accuracy: 0.43938112258911133\n",
      "Mackuasaurus\n",
      "Diiniandang\n",
      "Lepongsaurus\n",
      "Bosongus\n",
      "Owarasaurus\n",
      "Chinosuulus\n",
      "Ocerthisaurus\n",
      "\n",
      "\n",
      "Epoch 14 | Loss: 78.94987487792969 | Avr Loss: 1.6447890996932983 | Num of Batches: 48 | Accuracy: 0.4440482556819916\n",
      "Giocolosaurus\n",
      "Marbonicerythodrnasaurus\n",
      "Micanomilus\n",
      "Kagasaurus\n",
      "Irechoenur\n",
      "Negnicelosaurus\n",
      "Hegrosaurus\n",
      "\n",
      "\n",
      "Epoch 15 | Loss: 78.36012268066406 | Avr Loss: 1.632502555847168 | Num of Batches: 48 | Accuracy: 0.44817686080932617\n",
      "Pawoceratathe\n",
      "Malosaurus\n",
      "Arnwshangphus\n",
      "Dycandenosaurus\n",
      "Hypanotlemus\n",
      "Kelottiteo\n",
      "Peghoigos\n"
     ]
    }
   ],
   "source": [
    "X_padded = tf.keras.utils.pad_sequences(x_sequences[:], value=-1.0, padding='post', dtype='float32')\n",
    "Y_padded = tf.keras.utils.pad_sequences(y_sequences[:], value=-1.0, padding='post', dtype='float32')\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_padded, Y_padded)) \\\n",
    "    .batch(batch_size, drop_remainder=True) \\\n",
    "    .repeat() \\\n",
    "    .prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "recurrent_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "bias_initializer=tf.keras.initializers.Ones() \n",
    "\n",
    "inp = tf.keras.Input(shape=(None, n_vocab_size))\n",
    "x = tfl.Masking(mask_value=-1.)(inp)\n",
    "rnn_cell = tfl.SimpleRNNCell(\n",
    "    n_a,\n",
    "    kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "    recurrent_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "    bias_initializer=tf.keras.initializers.Ones()\n",
    ")\n",
    "x = tfl.RNN(rnn_cell, return_sequences=True) (x)\n",
    "out = tfl.Dense(n_vocab_size, activation=\"softmax\",\n",
    "                    kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                    bias_initializer=tf.keras.initializers.Ones())(x)\n",
    "custom_model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "# custom_model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, clipvalue=10.0)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "train_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    steps_per_epoch = X_padded.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        num_batches = 0\n",
    "        total_losses = []\n",
    "        \n",
    "        for step, (x_batch, y_batch) in enumerate(dataset):\n",
    "            if step >= steps_per_epoch:\n",
    "                break\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = custom_model(x_batch, training=True)\n",
    "                loss = loss_fn(y_batch, predictions)\n",
    "                train_acc.update_state(y_batch, predictions)\n",
    "                \n",
    "            grads = tape.gradient(loss, custom_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, custom_model.trainable_variables))\n",
    "            \n",
    "            total_losses.append(loss)\n",
    "            num_batches += 1\n",
    "\n",
    "        epoch_loss = tf.reduce_sum(total_losses)\n",
    "        print(f\"\\n\\nEpoch {epoch+1} | Loss: {epoch_loss} | Avr Loss: {epoch_loss/num_batches} | Num of Batches: {num_batches} | Accuracy: {train_acc.result().numpy()}\")\n",
    "\n",
    "        for i in range(7):\n",
    "            name = get_sample(custom_model)\n",
    "            print(name.title())\n",
    "\n",
    "train(train_dataset, 15)\n",
    "custom_model.save(\"new-dino_RNN_Masking_Batch_Trained.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a376f-2fa5-40a8-8fe3-26e29453cabe",
   "metadata": {},
   "source": [
    "## Sampling with RNN batch trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c5889e-f5ca-42c2-afbe-2cc4aa6e4f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = Tapusaucasaurus, len = 15\n",
      "name = Haenyceratops, len = 13\n",
      "name = Meptceratops, len = 12\n",
      "name = Udrazogyithus, len = 13\n",
      "name = Diohenator, len = 10\n",
      "name = Fursinia, len = 8\n",
      "name = Nonsygapesauruchus, len = 18\n",
      "name = Ceviliornphia, len = 13\n",
      "name = Kolovennasaurus, len = 15\n",
      "name = Mizangodon, len = 10\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"dino_RNN_Masking_Batch_Trained.keras\")\n",
    "\n",
    "for _ in range(10):\n",
    "    name = get_sample(loaded_model)\n",
    "    print(f\"name = {name.title()}, len = {len(name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d963f4-bf0b-4189-b11d-5e665f6e8fbd",
   "metadata": {},
   "source": [
    "# Model RNN with no masking & Train with SGD on raw X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a93027fb-6a31-41d1-8908-5e74c05b9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X) = <class 'list'>, len(X) = 1536\n",
      "type(Y) = <class 'list'>, len(Y) = 1536\n",
      "smoothLoss = 23.070858062030304\n",
      "\n",
      "\n",
      "Step: 1, Loss: 45.90201187133789, SmoothLoss: 23.09368896484375,  Accuracy: 0.1428571492433548\n",
      "Imfdllhutkoysfqlewpynwk\n",
      "Danakapnvoxaufwhws\n",
      "Pbcupntbvdog\n",
      "Vzhawjlxiyogbhyf\n",
      "Mutyujosbdsllijfpszodyciwhc\n",
      "Aee\n",
      "Ibzt\n",
      "\n",
      "\n",
      "Step: 10001, Loss: 26.91754913330078, SmoothLoss: 27.82469367980957,  Accuracy: 0.318737268447876\n",
      "Ojis\n",
      "Depsaus\n",
      "Telliaus\n",
      "Kemiserusiecanas\n",
      "Erus\n",
      "Us\n",
      "Taletosausamaurusalaur\n",
      "\n",
      "\n",
      "Step: 20001, Loss: 32.59544372558594, SmoothLoss: 27.557518005371094,  Accuracy: 0.3304867148399353\n",
      "Turuvhoneng\n",
      "S\n",
      "Ahaus\n",
      "Osua\n",
      "Tevin\n",
      "Lochausaus\n",
      "Osaus\n",
      "\n",
      "\n",
      "Step: 30001, Loss: 31.2158145904541, SmoothLoss: 27.494462966918945,  Accuracy: 0.33551132678985596\n",
      "S\n",
      "Rongonosanfxfusosuruhlirus\n",
      "Phianlanis\n",
      "Auraus\n",
      "Kanauceleloscandrur\n",
      "Tonkulopsangusalinusausaus\n",
      "Saurururusausbaurohangorunj\n",
      "\n",
      "\n",
      "Step: 35000, Loss: 29.461299896240234, SmoothLoss: 27.48915672302246,  Accuracy: 0.3371524512767792\n",
      "Ngoanyvaurhropocrusages\n",
      "Trosaror\n",
      "Dopsusan\n",
      "S\n",
      "Kathurys\n",
      "S\n",
      "Terussingncosaus\n"
     ]
    }
   ],
   "source": [
    "X = indexes_to_one_hot_vectors(inputs[:], n_vocab_size)\n",
    "Y = indexes_to_one_hot_vectors(outputs[:], n_vocab_size)\n",
    "\n",
    "print(f\"type(X) = {type(X)}, len(X) = {len(X)}\")\n",
    "print(f\"type(Y) = {type(Y)}, len(Y) = {len(Y)}\")\n",
    "\n",
    "# for x, y in zip(X, Y):\n",
    "#     for i in range(0, len(x)-1):\n",
    "#         all_equal = all(a == b for a, b in zip(x[i+1], y[i]))\n",
    "#         if not all_equal:\n",
    "#             raise ValueError(f\"x[{i+1}] = {x[i+1]}, y[{i}] = {y[i]}\")\n",
    "\n",
    "\n",
    "kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "recurrent_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "bias_initializer=tf.keras.initializers.Ones() \n",
    "\n",
    "inp = tf.keras.Input(shape=(None, n_vocab_size))\n",
    "rnn_cell = tfl.SimpleRNNCell(\n",
    "    n_a,\n",
    "    kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "    recurrent_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "    bias_initializer=tf.keras.initializers.Ones()\n",
    ")\n",
    "x = tfl.RNN(rnn_cell, return_sequences=True) (inp)\n",
    "out = tfl.Dense(n_vocab_size, activation=\"softmax\",\n",
    "                    kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                    bias_initializer=tf.keras.initializers.Ones())(x)\n",
    "custom_model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "# custom_model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, clipvalue=5.0)\n",
    "train_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "\n",
    "def train(X_train, Y_train, num_iterations):\n",
    "    smoothLoss = -np.log(1.0/n_vocab_size)*7\n",
    "    print(f\"smoothLoss = {smoothLoss}\")\n",
    "    \n",
    "    for step in range(num_iterations):\n",
    "        idx = step % len(X_train)\n",
    "        # x = tf.convert_to_tensor([X_train[idx]])\n",
    "        # y = tf.convert_to_tensor([Y_train[idx]])\n",
    "        \n",
    "        x = np.expand_dims(X_train[idx], axis=1)\n",
    "        y = np.expand_dims(Y_train[idx], axis=1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = custom_model(x, training=True)\n",
    "            loss = loss_fn(y, predictions)\n",
    "\n",
    "            # custom_loss = 0\n",
    "            # for t, pred in enumerate(predictions):\n",
    "            #     yt_id = np.argmax(y[t].ravel())\n",
    "            #     value = tf.reshape(pred, -1)[yt_id]\n",
    "            #     # print(f\"{t}: value = {value}\")\n",
    "            #     custom_loss -= np.log(value) \n",
    "            # print(f\"custom_loss = {custom_loss}, custom_loss / num = {custom_loss/len(predictions)}\")\n",
    "                \n",
    "            grads = tape.gradient(loss, custom_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, custom_model.trainable_variables))\n",
    "            train_acc.update_state(y, predictions)\n",
    "            \n",
    "            smoothLoss = smoothLoss * 0.999 + loss * 0.001\n",
    "\n",
    "            if step % 10_000 == 0 or step == num_iterations-1:\n",
    "                print(f\"\\n\\nStep: {step+1}, Loss: {loss}, SmoothLoss: {smoothLoss},  Accuracy: {train_acc.result().numpy()}\")\n",
    "                for i in range(7):\n",
    "                    name = get_sample(custom_model)\n",
    "                    print(name.title())\n",
    "\n",
    "train(X, Y, 35000)\n",
    "custom_model.save(\"new-dino_RNN_No_masking_SGD_Trained.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e156d6-04ff-4adc-93fd-bf84c3917b53",
   "metadata": {},
   "source": [
    "## Sampling with RNN SGD trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6dab602-9f61-4ced-bf97-f79cd8cfb72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = S, len = 1\n",
      "name = Husatraustenos, len = 14\n",
      "name = Sanes, len = 5\n",
      "name = Pocatopesenatos, len = 15\n",
      "name = S, len = 1\n",
      "name = Los, len = 3\n",
      "name = S, len = 1\n",
      "name = Qisaultosalokrusapichupus, len = 25\n",
      "name = Ylolerus, len = 8\n",
      "name = Ateysatosas, len = 11\n"
     ]
    }
   ],
   "source": [
    "sgd_model = tf.keras.models.load_model(\"dino_RNN_No_masking_SGD_Trained.keras\")\n",
    "\n",
    "for _ in range(10):\n",
    "    name = get_sample(sgd_model)\n",
    "    print(f\"name = {name.title()}, len = {len(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43cc4a-9614-4770-8c32-27971c649e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
